# Welcome to Kaggle NLP Competition Course - Evaluating Student Writing
https://www.kaggle.com/c/feedback-prize-2021


## Part I: Machine Learning Essentials

### Week 1 (Jan 16) basic python tutorial
- Video for Week 1 release
- [Key concepts in machine learning](https://towardsdatascience.com/machine-learning-basics-part-1-a36d38c7916)
- [How to win Kaggle competitions](https://docs.google.com/document/d/14KDMW_o1yflcZd4E0PSlKxzI68zdHG20Qz6X5wmkgSA/edit?usp=sharing)
- [Coding Environment](https://docs.google.com/presentation/d/1cYZACKaB7e2vRZAv8Oe1GcVy6U_xBeOoeptJsl3KZtI/edit?usp=sharing)
- [Python Tutorial #1: Basics Numpy](https://github.com/amenda860111/kaggle_nlp/blob/main/tutorial_1_basic_numpy.ipynb)
- [Python Tutorial #2: Basics Pandas](https://github.com/amenda860111/kaggle_nlp/blob/main/tutorial_2_basic_pandas.ipynb)
- Homework 1
	- coding homework from Python Tutorial #1 and #2 notebooks
	- install python, numpy and pandas 



### Week 2 (Jan 23) EDA & Feature Engineering
- Homework 1 due
- Online 1 hour office hour (9 am - 10 am, Beijing time)
- Video for Week 2 release
- [Python Tutorial #3: Basic data processing and visualization](https://github.com/amenda860111/kaggle_nlp/blob/main/tutorial_3_data_preprocessing_visualization.ipynb)
- [Feature Engineering](https://docs.google.com/presentation/d/13gwvLolY0Ug_WKROeVYpHpblWhNhvmj3DskSxsu3Ta0/edit#slide=id.ge645f5e39a_0_55)
- [Python Tutorial #4: Feature engineering](https://github.com/amenda860111/kaggle_nlp/blob/main/tutorial_4_feature_engineering.ipynb)
- Homework 2
	- coding homework from Python Tutorial #3 and #4 notebooks
	- install code environment using .yml file



### Week 3 (Jan 30) CV and linear models
- Homework 2 due
- Online 1 hour office hour (9 am - 10 am, Beijing time)
- Video for Week 3 release
- [Python Tutorial #5: Cross validation, grid search for parameter selection](https://github.com/amenda860111/kaggle_nlp/blob/main/tutorial_5_cross_validation.ipynb)
- [Python Tutorial #6: Linear regression, Ridge, Lasso models](https://github.com/amenda860111/kaggle_nlp/blob/main/tutorial_6_linear%20models.ipynb)
- Homework 3
	- coding homework from Python Tutorial #5 and #6 notebooks
	- sign in competition and successfully submit one public notebook 


### Week 4 (Feb 6) Tree based models, Xgboost and LightGBM
- Homework 3 due
- Online 1 hour office hour (9 am - 10 am, Beijing time)
- Video for Week 4 release
- [Python Tutorial #7: Decision Trees](https://github.com/amenda860111/kaggle_nlp/blob/main/tutorial_7_decision_tree.ipynb)
- [Gini impurity in decision tree CART algorithm](https://victorzhou.com/blog/gini-impurity/)
- [Python Tutorial #8: Xgboost & LightGBM](https://github.com/amenda860111/kaggle_nlp/blob/main/tutorial_8_xgboost_LightGBM.ipynb)
- [Intro to LightGBM & Xgboost](https://www.kaggle.com/prashant111/lightgbm-classifier-in-python)
- [Gradient boosting explained](https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/)
- Homework 4: write your own python notebook for one of the following two problems (train-test split, data cleaning as well as cross validation for hyperparameter search)
	- regression: predict median housing price (boston housing price data) using the linear models and the tree based models, compare results
	- classification: predict cancer or not (breast cancer data) using the linear models and the tree based models, compare results


### Week 5 (Feb 13) Deep neural network
- Homework 4 due
- Online 1 hour office hour (9 am - 10 am, Beijing time)
- Video for Week 5 release
- [DNN](https://docs.google.com/presentation/d/1X5zYyAD5rEJv6gQVaBy2i_0ti0_JOVGcUMwxBRnzwYE/edit#slide=id.gde0cd20ce7_0_76)
- [Python Tutorial #9: Deep NN](https://github.com/amenda860111/kaggle_nlp/blob/main/tutorial_9_deep_NN.ipynb)
- [Stacking models](https://docs.google.com/presentation/d/1DBp7sNM__CKd38C6QdATjQhdj9lK5DD6UkZqx2FZ7gI/edit#slide=id.gebaeaeb46d_0_39)
- [Python Tutorial #10: Stacking models by sklearn](https://github.com/amenda860111/kaggle_nlp/blob/main/tutorial_10_stacking_models_sklearn.ipynb)
- [Python Tutorial #11: Stacking models by any ML model](https://github.com/amenda860111/kaggle_nlp/blob/main/tutorial_11_stacking_models_by%20ML.ipynb)
- [Python Tutorial #12: Staching models by weighted average](https://github.com/amenda860111/kaggle_nlp/blob/main/tutorial_12_stacking_models_by_weighted_average.ipynb)
- Homework 5: 
	- For the regression or classification problem you chose from last week, implement a deep NN model
	- continue your work from last week, choose a method to stack the 8 models (7 from last week plus deep NN from this week), can you achieve a better result?

## Part II 
### Week 6 (Feb 20) NLP advanced topic
- Homework 5 due
- Online 3 hour class (9 am - 12 pm, Beijing time)
- [Transformer paper](https://arxiv.org/pdf/1706.03762.pdf)
- [Transformer model explained](https://jalammar.github.io/illustrated-transformer/)
- [Transformer Python Tutorial]()
- [Kaggle Strategy]()

- Video for Week 6 release

## Part III: Kaggle notebook & group sessions
### Week 7 (Feb 27)
- Online 3 hour class (9 am - 12 pm, Beijing time)
### Week 8 (March 6)
- Online 3 hour class (9 am - 12 pm, Beijing time)

## Entry Deadline March 8, 2022 
## Team Merger Deadline March 8, 2022

### Week 9 (March 13)
- Online 3 hour class (9 am - 12 pm, Beijing time)


## Submission Deadline March 15, 2022

